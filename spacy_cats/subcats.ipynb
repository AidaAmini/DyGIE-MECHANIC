{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c480925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "import pandas as pd\n",
    "\n",
    "from scispacy.linking import EntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a021b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLENDER_REL_FILE_LISTS = [\"genes_diseases_relation.csv\",\"chem_gene_ixns_relation.csv\", \"chemicals_diseases_relation.csv\" ]\n",
    "\n",
    "PATH_TO_DATA = \"../../../for_edan/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8701a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "# linker = EntityLinker(resolve_abbreviations=True, name=\"umls\")\n",
    "config = {\n",
    "    \"linker_name\": \"umls\",\n",
    "    \"resolve_abbreviations\": True\n",
    "}\n",
    "linker = nlp.add_pipe(\"scispacy_linker\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a848804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 1000\n",
      "count 2000\n",
      "count 3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Input: tui_filename (file name with tag to category name)\n",
    "Create dict of tag to category name based on file\n",
    "\"\"\"\n",
    "def read_type_ids(tui_filename):\n",
    "  tui_label_dict = {}\n",
    "  input_file = open(tui_filename)\n",
    "  for line in input_file:\n",
    "    line_parts = line[:-1].split(\"\\t\")\n",
    "    tui_label_dict[line_parts[0]] = line_parts[1]\n",
    "  return tui_label_dict\n",
    "\n",
    "\"\"\"\n",
    "Input: file_path (file name)\n",
    "copy lines of file to elements of list but remove newline\n",
    "\"\"\"\n",
    "def read_temp_file(file_path):\n",
    "  input_file = open(file_path)\n",
    "  res = []\n",
    "  for line in input_file:\n",
    "    res.append(line[:-1])\n",
    "  return res\n",
    "\n",
    "\"\"\"\n",
    "Depricated\n",
    "\"\"\"\n",
    "def write_cofie_entities(cofie_kb_path):\n",
    "    seen_spans = []\n",
    "    output_file = open(PATH_TO_DATA + \"cofie_spans.txt\", \"w\")\n",
    "    input_file = open(cofie_kb_path)\n",
    "    count = 0\n",
    "    for line in input_file:\n",
    "\n",
    "      if count % 10000 == 0:\n",
    "        print(count)\n",
    "      count += 1\n",
    "      \n",
    "      if count == 1:\n",
    "        continue\n",
    "      line_parts = line[:-1].split(\"\\t\")\n",
    "      # import pdb; pdb.set_trace()\n",
    "      for i in [2,3]:\n",
    "        if line_parts[i] not in seen_spans:\n",
    "          output_file.write(line_parts[i] + '\\n')\n",
    "          seen_spans.append(line_parts[i])\n",
    "          \n",
    "\"\"\"\n",
    "Depricated\n",
    "\"\"\"\n",
    "def write_blender_entities(blender_kb_path):\n",
    "    # seen_spans = []\n",
    "    seen_spans = read_temp_file(\"blender_spans_temp.txt\")\n",
    "    output_file = open(\"blender_spans.txt\", \"w\")\n",
    "    for span in seen_spans:\n",
    "      output_file.write(span + '\\n')\n",
    "    for file in BLENDER_REL_FILE_LISTS:\n",
    "      df = pd.read_csv(blender_kb_path + file, header=0, sep=\"\\t\")\n",
    "\n",
    "      for row in df.iterrows():\n",
    "        if file == \"genes_diseases_relation.csv\" and int(row[0]) < 43700000:\n",
    "          continue\n",
    "        if row[0] % 100000 == 0:\n",
    "          print(file)\n",
    "          print(str(row[0]))\n",
    "        if file == \"genes_diseases_relation.csv\":\n",
    "          span1 = str(row[1][\"GeneSymbol\"]).lower()\n",
    "          span2 = str(row[1][\"DiseaseName\"]).lower()\n",
    "        elif file == \"chem_gene_ixns_relation.csv\":\n",
    "          span1 = str(row[1][\"ChemicalName\"]).lower()\n",
    "          span2 = str(row[1][\"GeneName\"]).lower()\n",
    "        else:\n",
    "          span1 = str(row[1][\"ChemicalName\"]).lower()\n",
    "          span2 = str(row[1][\"DiseaseName\"]).lower()\n",
    "\n",
    "        if span1 not in seen_spans:\n",
    "          seen_spans.append(span1)\n",
    "          output_file.write(span1 + '\\n')\n",
    "\n",
    "        if span2 not in seen_spans:\n",
    "          seen_spans.append(span2)\n",
    "          output_file.write(span2 + '\\n')\n",
    "\n",
    " \n",
    "\"\"\"\n",
    "Input: input_filename spans extracted from KB\n",
    "       output_filename TODO figure out\n",
    "       \n",
    "\"\"\"\n",
    "def get_types(input_filename, output_filename):\n",
    "    type_count_dict = {}\n",
    "    log_file = open(PATH_TO_DATA + \"training_spans_subcats.tsv\", \"w\")\n",
    "    output_file = open(output_filename, \"w\")\n",
    "    # This line takes a while, because we have to download ~1GB of data\n",
    "    # and load a large JSON file (the knowledge base). Be patient!\n",
    "    # Thankfully it should be faster after the first time you use it, because\n",
    "    # the downloads are cached.\n",
    "    # NOTE: The resolve_abbreviations parameter is optional, and requires that\n",
    "    # the AbbreviationDetector pipe has already been added to the pipeline. Adding\n",
    "    # the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n",
    "    # that linking will only be performed on the long form of abbreviations.\n",
    "    tui_label_dict = read_type_ids(PATH_TO_DATA + \"tui_labels.tsv\")\n",
    "    input_file = open(input_filename)\n",
    "    count = 0\n",
    "    for line in input_file:\n",
    "      count += 1\n",
    "      if count % 1000 == 0:\n",
    "        print(\"count \" + str(count))\n",
    "\n",
    "      # extract key entities\n",
    "      doc = nlp(line[:-1])\n",
    "      if len(doc.ents) != 0:\n",
    "        for i in range(len(doc.ents)):\n",
    "          entity = doc.ents[i]\n",
    "          for umls_ent in entity._.kb_ents:\n",
    "            types_list = linker.kb.cui_to_entity[umls_ent[0]].types\n",
    "            # Count occurences of each TUI in dictionary\n",
    "            for _type in types_list:\n",
    "              if _type not in type_count_dict:\n",
    "                type_count_dict[_type] = 0\n",
    "              type_count_dict[_type] += 1\n",
    "              # import pdb; pdb.set_trace()\n",
    "              log_file.write(line[:-1] + \"\\t\" + tui_label_dict[_type]+ \"\\n\")\n",
    "    output_file = open(output_filename, \"w\")\n",
    "    for _type in type_count_dict:\n",
    "      output_file.write(_type + \"\\t\" + str(type_count_dict[_type]) + '\\n')\n",
    "\n",
    "      \n",
    "\n",
    "# write_cofie_entities(\"/home/aida/covid_clean/dygiepp/complete_KB_coref.tsv\")\n",
    "# write_blender_entities(\"/data/aida/\")\n",
    "#get_types(\"blender_spans2.txt\", \"blender_spans_tags.txt\")\n",
    "doc = get_types(PATH_TO_DATA + \"training_spans.txt\", PATH_TO_DATA + \"training_type_count.tsv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06135f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'alias_to_cuis',\n",
       " 'cui_to_entity',\n",
       " 'semantic_type_tree']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(linker.kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6733e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fad118c6f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ddd0a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['get', 'has', 'kb_ents', 'set', 'umls_ents']\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents[0]._.umls_ents)\n",
    "print(dir(doc.ents[0]._))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b33ef89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'long - term imaging follow - up after flow - diverter implantation\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "786e4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "           inherited motor neuron disease caused by the expansion \\\n",
    "           of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "           SBMA can be caused by this easily\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "faf37e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0921346 , -0.24784482,  0.7017714 ,  0.3013472 ,  0.04505855,\n",
       "       -1.0176959 ,  0.7290837 , -1.083273  , -0.80804557, -0.1108322 ,\n",
       "       -0.46059218,  1.1197779 ,  1.5859103 , -1.5850055 , -0.19468474,\n",
       "        0.86162543, -1.1898143 ,  0.4025337 ,  0.546126  ,  0.3930942 ,\n",
       "       -0.5489894 ,  0.07679972, -1.3166326 , -0.6879619 ,  0.7433906 ,\n",
       "        0.6149941 ,  0.152344  , -0.32041124, -0.47136348,  0.50137335,\n",
       "        0.11929529, -0.28746825,  0.21233952, -1.5002258 , -0.53008753,\n",
       "       -0.5166469 , -0.98063946, -1.1089299 ,  1.5003526 , -0.0623616 ,\n",
       "       -0.63929164, -0.12292683, -0.16088364, -0.6792747 ,  0.1937968 ,\n",
       "        0.83047587,  1.5787429 ,  0.87463486, -0.15952143,  0.85969055,\n",
       "        0.36707333, -0.29322675, -0.29202163, -0.17316845, -0.82743335,\n",
       "        0.32459146,  0.9104742 , -0.12151811,  0.40474847,  0.43918195,\n",
       "        1.6888247 , -1.3032068 ,  0.97610736, -0.7258777 ,  0.10229971,\n",
       "        0.1360836 , -1.1312985 , -0.7466174 ,  0.78176427,  1.4938235 ,\n",
       "       -0.41241103,  0.04692579,  1.8949723 , -1.5961593 , -0.90128446,\n",
       "       -0.02151091,  1.4605201 ,  0.5000098 , -0.42159322, -0.8843557 ,\n",
       "       -0.23486057, -0.21491979, -1.6321315 ,  0.4259053 ,  0.33160558,\n",
       "       -0.5852361 ,  0.9976858 ,  0.08535436,  0.07546201,  0.6538155 ,\n",
       "        0.02140495,  0.41023806, -0.38908476, -0.9317764 , -0.8743636 ,\n",
       "       -0.5058605 ], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce9eee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C0521329', 1.0)\n",
      "('C0037922', 0.8068786859512329)\n",
      "('C3887662', 0.7646855711936951)\n",
      "('C1334264', 0.7219825983047485)\n",
      "('C0037925', 0.7107230424880981)\n",
      "('C1947952', 1.0)\n",
      "('C0032372', 0.7856597900390625)\n",
      "('C2586323', 0.7522947192192078)\n",
      "('C1744560', 0.7451043128967285)\n",
      "('C0030442', 0.723750114440918)\n",
      "('C0026846', 1.0)\n",
      "('C0541794', 1.0)\n",
      "('C0026847', 0.8910943865776062)\n",
      "('C1848736', 0.884716272354126)\n",
      "('C0043116', 0.8696750998497009)\n",
      "('C1705240', 0.9999998807907104)\n",
      "('C1839259', 0.9999998807907104)\n",
      "('C0439660', 1.0)\n",
      "('C0019247', 0.8003369569778442)\n",
      "('C4277541', 0.7369471192359924)\n",
      "('C4277511', 0.735810399055481)\n",
      "('C0598589', 0.7073548436164856)\n",
      "('C0085084', 1.0)\n",
      "('C1865412', 0.8891978859901428)\n",
      "('C0007595', 0.8664658665657043)\n",
      "('C4761515', 0.8664658665657043)\n",
      "('C1654621', 0.8529700636863708)\n",
      "('C0196940', 0.8281357884407043)\n",
      "('C1516670', 0.8198980689048767)\n",
      "('C0032500', 0.7472081780433655)\n",
      "('C0392213', 0.7402303218841553)\n",
      "('C0034786', 1.0)\n",
      "('C1367578', 1.0)\n",
      "('C1447749', 1.0)\n",
      "('C4764257', 0.8611212968826294)\n",
      "('C1323350', 0.848429799079895)\n",
      "('C0003504', 1.0)\n",
      "('C0003761', 1.0)\n",
      "('C0003790', 1.0)\n",
      "('C0332284', 1.0)\n",
      "('C0559546', 1.0)\n",
      "('C1705240', 0.9999998807907104)\n",
      "('C1839259', 0.9999998807907104)\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    for umls_ent in ent._.kb_ents:\n",
    "        print(umls_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dee659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b27d096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc67397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Organism\").ents[0].similarity(nlp(\"Organism\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "772f4d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edan/miniconda3/envs/spacy/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Span.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.719016"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Organism\").ents[0].similarity(nlp(\"Animal\").ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dc5831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edan/miniconda3/envs/spacy/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "tui_label_dict = read_type_ids(PATH_TO_DATA + \"tui_labels.tsv\")\n",
    "categories = [\"Animal\", \"Organism\", \"Drug\", \"Cell\", \"Behavior\", \"Organization\",\n",
    "              \"Group\", \"Disease\", \"Symptom\", \"Body Part\", \"Organ\", \"Medical Device\",\n",
    "              \"Product\", \"Concept\", \"Finding\", \"Procedure\", \"Molecule\", \"Organic Chemical\", \"Virus\"]\n",
    "\n",
    "output_file = open(\"categories_sm.tsv\", \"w\")\n",
    "def get_cat(doc):\n",
    "    top_cat = \"None\"\n",
    "    top_prob = 0\n",
    "    for cat in categories:\n",
    "        doc2 = nlp(cat)\n",
    "        prob = doc.similarity(doc2)\n",
    "        if (top_prob < prob):\n",
    "            top_cat = cat\n",
    "            top_prob = prob\n",
    "    assert top_cat != \"None\"\n",
    "    return top_cat, top_prob\n",
    "    \n",
    "\n",
    "for value in tui_label_dict.values():\n",
    "    doc = nlp(value)\n",
    "    cat, prob = get_cat(doc)\n",
    "    output_file.write(value + \"\\t\" + cat + \"\\t\" + str(prob) + \"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77c80897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23025123493015764"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27dc8a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1925040384439355"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc.vector.dot(doc2.vector) / doc.vector_norm / doc2.vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be9f6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_get_array_attrs',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f190d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
